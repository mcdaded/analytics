{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class MDP(object):\n",
    "  \"\"\" \n",
    "    Defines an Markov Decision Process containing:\n",
    "  \n",
    "    - States, s \n",
    "    - Actions, a\n",
    "    - Rewards, r(s,a)\n",
    "    - Transition Matrix, t(s,a,_s)\n",
    "\n",
    "    Includes a set of abstract methods for extended class will\n",
    "    need to implement.\n",
    "\n",
    "  \"\"\"\n",
    " \n",
    "  def __init__(self, states=None, actions=None, rewards=None, transitions=None, \n",
    "        discount=.99, tau=.01, epsilon=.01):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    states: 1-D array\n",
    "        The states of the environment\n",
    "\n",
    "    actions: 1-D array\n",
    "        The possible actions by the agent.\n",
    "\n",
    "    rewards: 2-D array\n",
    "        The rewards corresponding to each action at each state of the environment.\n",
    "\n",
    "    transitions: 2-D array\n",
    "        The transition probabilities between the states of the environment.\n",
    "\n",
    "    discount: float\n",
    "        The discount rate for the reward.\n",
    "    \"\"\"    \n",
    "    self.s = np.array(states)\n",
    "    self.a = np.array(actions)\n",
    "    self.r = np.array(rewards)\n",
    "    self.t = np.array(transitions)\n",
    "    \n",
    "    self.discount = discount\n",
    "    self.tau = tau\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    # Value iteration will update this\n",
    "    self.values = None\n",
    "    self.policy = None\n",
    "\n",
    "  def getTransitionStatesAndProbs(self, state, action):\n",
    "    \"\"\"\n",
    "      Returns the list of transition probabilities\n",
    "    \"\"\"\n",
    "    return self.t[state][action][:]\n",
    "\n",
    "  def getReward(self, state):\n",
    "    \"\"\"\n",
    "      Gets reward for transition from state->action->nextState.\n",
    "    \"\"\"\n",
    "    return self.r[state]\n",
    "\n",
    "  def takeAction(self, state, action):\n",
    "    \"\"\"\n",
    "      Take an action in an MDP, return the next state\n",
    "\n",
    "      Chooses according to probability distribution of state transitions,\n",
    "      contingent on actions.\n",
    "    \"\"\"\n",
    "    return np.random.choice(self.s, p=self.getTransitionStatesAndProbs(state, action)) \n",
    "\n",
    "  def valueIteration(self):\n",
    "    \"\"\"\n",
    "      Performs value iteration to populate the values of all states in\n",
    "      the MDP. \n",
    "    \"\"\"\n",
    "    # Initialize V_0 to zero\n",
    "    self.values = np.zeros(len(self.s))\n",
    "    self.policy = np.zeros([len(self.s), len(self.a)])\n",
    "    policy_switch = 0\n",
    "    # Loop until convergence\n",
    "    while True:\n",
    "      # To be used for convergence check\n",
    "      oldValues = np.copy(self.values)\n",
    "      for i in range(len(self.s)-1):\n",
    "        self.values[i] = self.r[i] + np.max(self.discount * \\\n",
    "              np.dot(self.t[i][:][:], self.values))\n",
    "      # Check Convergence\n",
    "      if np.max(np.abs(self.values - oldValues)) <= self.epsilon:\n",
    "        break\n",
    "\n",
    "  def extractPolicy(self):\n",
    "    \"\"\"\n",
    "      Extract policy from values after value iteration runs.\n",
    "    \"\"\"\n",
    "    self.policy = np.zeros([len(self.s),len(self.a)])\n",
    "    for i in range(len(self.s)-1):\n",
    "      state_policy = np.zeros(len(self.a))\n",
    "      state_policy = self.r[i] + self.discount* \\\n",
    "            np.dot(self.t[i][:][:], self.values)\n",
    "      # Softmax the policy \n",
    "      state_policy -= np.max(state_policy)\n",
    "      state_policy = np.exp(state_policy / float(self.tau))\n",
    "      state_policy /= state_policy.sum()\n",
    "      self.policy[i] = state_policy\n",
    "\n",
    "  def simulate(self, state):\n",
    "    \"\"\" \n",
    "      Runs the solver for the MDP, conducts value iteration, extracts policy,\n",
    "      then runs simulation of problem.\n",
    "\n",
    "      NOTE: Be sure to run value iteration (solve values for states) and to\n",
    "       extract some policy (fill in policy vector) before running simulation\n",
    "    \"\"\"\n",
    "    # Run simulation using policy until terminal condition met    \n",
    "    while not self.isTerminal(state):\n",
    "      # Determine which policy to use (non-deterministic)\n",
    "      policy = self.policy[np.where(self.s == state)[0][0]]\n",
    "      p_policy = self.policy[np.where(self.s == state)[0][0]] / \\\n",
    "            self.policy[np.where(self.s == state)[0][0]].sum()\n",
    "\n",
    "      # Get the parameters to perform one move\n",
    "      stateIndex = np.where(self.s == state)[0][0]\n",
    "      policyChoice = np.random.choice(policy, p=p_policy)\n",
    "      actionIndex = np.random.choice(np.array(np.where(self.policy[state][:] == policyChoice)).ravel())\n",
    "\n",
    "      # Take an action, move to next state\n",
    "      nextState = self.takeAction(stateIndex, actionIndex)\n",
    "\n",
    "      print(\"In state: {}, taking action: {}, moving to state: {}\".format(\n",
    "        state, self.a[actionIndex], nextState))\n",
    "\n",
    "      # End game if terminal state reached\n",
    "      state = int(nextState)\n",
    "      if self.isTerminal(state):\n",
    "        # print \"Terminal state: {} has been reached. Simulation over.\".format(state)\n",
    "        return state\n",
    "\n",
    "\n",
    "class BettingGame(MDP):\n",
    "  \"\"\"\n",
    "    Defines the Betting Game:\n",
    "\n",
    "    Problem: A gambler has the chance to make bets on the outcome of\n",
    "    a fair coin flip. If the coin is heads, the gambler wins as many\n",
    "    dollars back as was staked on that particular flip - otherwise\n",
    "    the money is lost. The game is won if the gambler obtains $100,\n",
    "    and is lost if the gambler runs out of money (has 0$). This gambler\n",
    "    did some research on MDPs and has decided to enlist them to assist\n",
    "    in determination of how much money should be bet on each turn. Your\n",
    "    task is to build that MDP!\n",
    "\n",
    "    Params:\n",
    "\n",
    "        pHead: Probability of coin flip landing on heads\n",
    "          - Use .5 for fair coin, else choose a bias [0,1]\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, pHeads=.5, discount=.99, epsilon=.1, tau=.0001):\n",
    "    MDP.__init__(self,discount=discount,tau=tau,epsilon=epsilon)\n",
    "    self.pHeads = pHeads\n",
    "    self.setBettingGame(pHeads)\n",
    "    self.valueIteration()\n",
    "    self.extractPolicy()\n",
    "\n",
    "    # Edge case fix: Policy for $1\n",
    "    self.policy[1][:] = 0\n",
    "    self.policy[1][1] = 1.0\n",
    "\n",
    "  def isTerminal(self, state):\n",
    "    \"\"\"\n",
    "      Checks if MDP is in terminal state.\n",
    "    \"\"\"\n",
    "    return True if state is 100 or state is 0 else False\n",
    "\n",
    "  def setBettingGame(self, pHeads=.5):\n",
    "    \"\"\"\n",
    "      Initializes the MDP to the starting conditions for\n",
    "      the betting game.\n",
    "\n",
    "      Params:\n",
    "        pHeads = Probability that coin lands on head\n",
    "          - .5 for fair coin, otherwise choose bias\n",
    "\n",
    "    \"\"\"\n",
    "    # This is how much we're starting with\n",
    "    self.pHeads = pHeads\n",
    "\n",
    "    # Initialize all possible states\n",
    "    self.s = np.arange(102)\n",
    "\n",
    "    # Initialize possible actions\n",
    "    self.a = np.arange(101)\n",
    "\n",
    "    # Initialize rewards\n",
    "    self.r = np.zeros(101)\n",
    "    self.r[0] = -5\n",
    "    self.r[100] = 10\n",
    "\n",
    "    # Initialize transition matrix\n",
    "    temp = np.zeros([len(self.s),len(self.a),len(self.s)])\n",
    "\n",
    "    # List comprehension using tHelper to determine probabilities for each index\n",
    "    self.t = [self.tHelper(i[0], i[1], i[2], self.pHeads) for i,x in np.ndenumerate(temp)]\n",
    "    self.t = np.reshape(self.t, np.shape(temp))\n",
    "\n",
    "    for x in range(len(self.a)):\n",
    "    # Remembr to add -1 to value it, and policy extract\n",
    "      # Send the end game states to the death state!\n",
    "      self.t[100][x] = np.zeros(len(self.s))\n",
    "      self.t[100][x][101] = 1.0\n",
    "      self.t[0][x] = np.zeros(len(self.s))\n",
    "      self.t[0][x][101] = 1.0\n",
    "\n",
    "  def tHelper(self, x, y, z , pHeads):\n",
    "    \"\"\"\n",
    "      Helper function to be used in a list comprehension to quickly\n",
    "      generate the transition matrix. Encodes the necessary conditions\n",
    "      to compute the necessary probabilities.\n",
    "\n",
    "      Params:\n",
    "      x,y,z indices\n",
    "      pHeads = probability coin lands on heads\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # If you bet no money, you will always have original amount\n",
    "    if x + y is z and y is 0:\n",
    "      return 1.0\n",
    "\n",
    "    # If you bet more money than you have, no chance of any outcome\n",
    "    elif y > x and x is not z:\n",
    "      return 0\n",
    "\n",
    "    # If you bet more money than you have, returns same state with 1.0 prob.\n",
    "    elif y > x and x is z:\n",
    "      return 1.0\n",
    "\n",
    "    # Chance you lose\n",
    "    elif x - y is z:\n",
    "      return 1.0 - pHeads\n",
    "\n",
    "    # Chance you win\n",
    "    elif x + y is z:\n",
    "      return pHeads\n",
    "\n",
    "    # Edge Case: Chance you win, and winnings go over 100\n",
    "    elif x + y > z and z is 100:\n",
    "      return pHeads\n",
    "\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "betting_game = BettingGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
